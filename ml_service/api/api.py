# ========================
# IMPORT MODULES
# ========================
from urllib.parse import urlparse

import torch
import json
import requests
import tempfile
import uuid
import os  #

import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any


try:
    from src import config
    from src.predict import Predictor
except ImportError as e:
    print("\nüî• L·ªñI IMPORT SRC !!!")
    print("L·ªói n√†y x·∫£y ra n·∫øu b·∫°n b·∫•m 'Play' tr·ª±c ti·∫øp tr√™n file n√†y.")
    print("H√£y ch·∫°y b·∫±ng C·∫•u h√¨nh Run 'uvicorn' c·ªßa PyCharm ho·∫∑c l·ªánh terminal.")
    print("Chi ti·∫øt:", e)
    raise

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# ========================
# DATA MODELS
# ========================

class PostInput(BaseModel):
    post_id: str
    text_content: str
    image_urls: List[str] = []

class BatchInput(BaseModel):
    posts: List[PostInput]

class PredictionResult(BaseModel):
    predicted_labels: List[str]
    all_probabilities: Dict[str, float]

class BatchResponseItem(BaseModel):
    post_id: str
    result: PredictionResult

class BatchResponse(BaseModel):
    predictions: List[BatchResponseItem]

# ========================
# LOAD MODEL ONCE
# ========================

print("--- Kh·ªüi ƒë·ªông ML Service ---")

# L·∫•y ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c g·ªëc (ml_service)
# Gi·∫£ s·ª≠ file n√†y ·ªü ml_service/api/api.py
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(ROOT_DIR, "models", "best_model.pth")

if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y model t·∫°i: {MODEL_PATH}")

predictor = Predictor(model_path=MODEL_PATH)

app = FastAPI(title="ML Classification Service")

print("--- ML Service s·∫µn s√†ng ---")

# ========================
# API ENDPOINTS
# ========================

@app.get("/")
def root():
    return {"status": "ML service OK"}

@app.post("/predict_batch", response_model=BatchResponse)
async def predict_batch(batch_input: BatchInput):
    results = []

    with tempfile.TemporaryDirectory() as temp_dir:
        for post in batch_input.posts:
            local_image_paths = []

            for url in post.image_urls:
                try:
                    response = requests.get(url, timeout=10, headers=HEADERS)
                    response.raise_for_status()

                    # Code m·ªõi (ƒê√£ s·ª≠a)
                    parsed_url = urlparse(url)  # T√°ch URL ra
                    clean_path = parsed_url.path  # Ch·ªâ l·∫•y ph·∫ßn ƒë∆∞·ªùng d·∫´n, b·ªè qua ph·∫ßn ?query...
                    ext = os.path.splitext(clean_path)[1] or ".jpg"  # L·∫•y ƒëu√¥i t·ª´ ƒë∆∞·ªùng d·∫´n s·∫°ch

                    filename = f"{uuid.uuid4()}{ext}"
                    save_path = os.path.join(temp_dir, filename)

                    with open(save_path, "wb") as f:
                        f.write(response.content)

                    local_image_paths.append(save_path)


                except requests.exceptions.RequestException as e:
                    print("L·ªói t·∫£i ·∫£nh:", url, e)

            # S·ª≠a l·ªói c·∫£nh b√°o `weights_only` c·ªßa PyTorch
            prediction = predictor.predict(
                text_content=post.text_content,
                image_paths=local_image_paths,
                threshold=0.5
            )

            results.append(BatchResponseItem(
                post_id=post.post_id,
                result=prediction
            ))

    return BatchResponse(predictions=results)

if __name__ == "__main__":
    print("--- CH·∫†Y TR·ª∞C TI·∫æP (B·∫±ng c√°ch b·∫•m Play ‚ñ∂Ô∏è) ---")
    uvicorn.run(app, host="0.0.0.0", port=8000)